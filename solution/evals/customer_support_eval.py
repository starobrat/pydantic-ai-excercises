"""
Evaluations for Customer Support Agent.

This module tests the agent's ability to:
1. Handle order management requests
2. Answer technical FAQ questions about robots
3. Provide helpful and professional responses
"""

import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from pydantic_evals import Case, Dataset
from pydantic_evals.evaluators import LLMJudge, IsInstance
from dotenv import load_dotenv
from customer_support_agent import run_agent, SupportResponse

load_dotenv()


# === TEST CASES ===

customer_support_dataset = Dataset(
    cases=[
        # Order Management Cases
        Case(
            name='status_zamowienia',
            inputs='Gdzie jest moja paczka? ZamÃ³wienie abc12345, uÅ¼ytkownik jan_kowalski',
            metadata={'category': 'order_status'},
        ),
        Case(
            name='anulowanie_zamowienia',
            inputs='ChcÄ™ anulowaÄ‡ zamÃ³wienie xyz98765. Jestem piotr_nowak. PowÃ³d: zmiana decyzji',
            metadata={'category': 'order_cancel'},
        ),
        Case(
            name='utworzenie_zamowienia',
            inputs='ChcÄ™ zamÃ³wiÄ‡ 2 roboty spawalnicze. Jestem uÅ¼ytkownikiem anna_wisniewska',
            metadata={'category': 'order_create'},
        ),
        
        # FAQ/RAG Cases - Technical Questions
        Case(
            name='faq_kalibracja',
            inputs='Jak skalibrowaÄ‡ ramiÄ™ robota?',
            metadata={'category': 'faq_technical'},
        ),
        Case(
            name='faq_przegrzewanie',
            inputs='Robot siÄ™ przegrzewa podczas pracy. Co mam zrobiÄ‡?',
            metadata={'category': 'faq_technical'},
        ),
        Case(
            name='faq_blad_systemu',
            inputs='WyÅ›wietla siÄ™ bÅ‚Ä…d E-101 na panelu robota',
            metadata={'category': 'faq_technical'},
        ),
        
        # Mixed/General Cases
        Case(
            name='brak_danych',
            inputs='ChcÄ™ sprawdziÄ‡ status zamÃ³wienia',
            metadata={'category': 'incomplete_request'},
        ),
    ],
    evaluators=[
        # Basic type check
        IsInstance(type_name='SupportResponse'),
        
        # Professional tone evaluation
        LLMJudge(
            rubric='''
            OceÅ„ czy odpowiedÅº jest uprzejma i profesjonalna.
            OdpowiedÅº powinna:
            1. ByÄ‡ napisana w grzecznym, pomocnym tonie
            2. Nie zawieraÄ‡ niegrzecznych lub lekcewaÅ¼Ä…cych sformuÅ‚owaÅ„
            3. TraktowaÄ‡ klienta z szacunkiem
            ''',
            include_input=True,
            assertion={'evaluation_name': 'professional_tone'},
        ),
        
        # Helpfulness evaluation
        LLMJudge(
            rubric='''
            OceÅ„ czy odpowiedÅº jest pomocna i zawiera konkretne informacje.
            OdpowiedÅº powinna:
            1. BezpoÅ›rednio odpowiadaÄ‡ na pytanie klienta
            2. ZawieraÄ‡ konkretne instrukcje lub informacje
            3. JeÅ›li brakuje danych - prosiÄ‡ o ich uzupeÅ‚nienie
            4. Nie byÄ‡ ogÃ³lnikowa ani wymijajÄ…ca
            ''',
            include_input=True,
            score={'evaluation_name': 'helpfulness'},
            assertion=False,
        ),
        
        # Completeness evaluation
        LLMJudge(
            rubric='''
            OceÅ„ kompletnoÅ›Ä‡ odpowiedzi.
            Dla pytaÅ„ o zamÃ³wienia:
            - OdpowiedÅº powinna zawieraÄ‡ informacje o statusie lub potwierdzenie akcji
            - JeÅ›li brakuje danych, powinna jasno wskazaÄ‡ jakich
            
            Dla pytaÅ„ technicznych o roboty:
            - OdpowiedÅº powinna zawieraÄ‡ konkretne instrukcje lub rozwiÄ…zania
            - Powinna byÄ‡ oparta na wiedzy technicznej (z FAQ)
            ''',
            include_input=True,
            score={'evaluation_name': 'completeness'},
            assertion=False,
        ),
    ],
)


# === ADDITIONAL DETAILED EVALUATION ===

detailed_faq_dataset = Dataset(
    cases=[
        Case(
            name='faq_kalibracja_detailed',
            inputs='Jak skalibrowaÄ‡ ramiÄ™ robota przemysÅ‚owego?',
        ),
        Case(
            name='faq_serwis',
            inputs='Kiedy powinienem przeprowadziÄ‡ serwis robota?',
        ),
        Case(
            name='faq_bezpieczenstwo',
            inputs='Jakie sÄ… zasady bezpieczeÅ„stwa przy pracy z robotem?',
        ),
    ],
    evaluators=[
        IsInstance(type_name='SupportResponse'),
        
        # Technical accuracy
        LLMJudge(
            rubric='''
            OdpowiedÅº na pytanie techniczne powinna:
            1. ZawieraÄ‡ konkretne kroki lub instrukcje
            2. ByÄ‡ technicznie sensowna (nawet jeÅ›li ogÃ³lna)
            3. Nie zawieraÄ‡ bÅ‚Ä™dnych lub niebezpiecznych porad
            ''',
            include_input=True,
            assertion={'evaluation_name': 'technical_accuracy'},
        ),
        
        # Actionability score
        LLMJudge(
            rubric='''
            OceÅ„ czy klient moÅ¼e podjÄ…Ä‡ konkretne dziaÅ‚ania na podstawie odpowiedzi.
            Wysoka ocena: odpowiedÅº zawiera jasne, wykonalne kroki
            Niska ocena: odpowiedÅº jest ogÃ³lnikowa, bez konkretnych instrukcji
            ''',
            include_input=True,
            score={'evaluation_name': 'actionability'},
            assertion=False,
        ),
    ],
)


def run_evaluation():
    """Run the main evaluation dataset."""
    print("=" * 60)
    print("ğŸ” EWALUACJA AGENTA OBSÅUGI KLIENTA")
    print("=" * 60)
    print("\nğŸ“‹ GÅ‚Ã³wny dataset ewaluacyjny\n")
    
    report = customer_support_dataset.evaluate_sync(run_agent)
    
    report.print(
        include_input=True,
        include_output=True,
        include_reasons=True,
    )
    
    return report


def run_detailed_faq_evaluation():
    """Run detailed FAQ evaluation."""
    print("\n" + "=" * 60)
    print("ğŸ”¬ SZCZEGÃ“ÅOWA EWALUACJA FAQ")
    print("=" * 60 + "\n")
    
    report = detailed_faq_dataset.evaluate_sync(run_agent)
    
    report.print(
        include_input=True,
        include_output=True,
        include_reasons=True,
    )
    
    return report


def main():
    """Run all evaluations."""
    print("\nğŸš€ Rozpoczynam ewaluacjÄ™ agenta obsÅ‚ugi klienta...\n")
    
    # Main evaluation
    main_report = run_evaluation()
    
    # Detailed FAQ evaluation
    faq_report = run_detailed_faq_evaluation()
    
    print("\n" + "=" * 60)
    print("âœ… EWALUACJA ZAKOÅƒCZONA")
    print("=" * 60)


if __name__ == "__main__":
    main()
